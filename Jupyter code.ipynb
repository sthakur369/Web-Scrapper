{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefe3701",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7430818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7126e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get City Nammes::\n",
    "\n",
    "# def get_city_names():\n",
    "#     global city_names\n",
    "#     url = 'https://www.makaan.com/price-trends'\n",
    "#     data = requests.get(url).text\n",
    "#     response = requests.get(url)\n",
    "#     city_names = []\n",
    "#     city_names.clear()\n",
    "\n",
    "#     # Create a BeautifulSoup object to parse the HTML content\n",
    "#     soup = BeautifulSoup(data, 'html.parser')\n",
    "#     table = soup.find('table', {'class': 'tbl', 'data-trend-type': 'apartment'})\n",
    "\n",
    "#     if table:\n",
    "#         for row in table.tbody.find_all('tr'):\n",
    "#             columns = row.find_all('td')\n",
    "#             first_column = columns[0].get_text(strip=True)\n",
    "#             city_names.append(first_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5a46d",
   "metadata": {},
   "source": [
    "# Get URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788c3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a GET request to the webpage\n",
    "def get_urls():\n",
    "    global city_names\n",
    "    urls = []\n",
    "    urls.clear()\n",
    "\n",
    "    base_url = 'https://www.makaan.com/price-trends'\n",
    "    city_names = ['Chennai', 'Mumbai', 'Pune', 'Puri', 'Bangalore']\n",
    "\n",
    "    for i in range(0,len(city_names)):\n",
    "        urls.append(base_url + '/property-rates-for-buy-in-' + city_names[i])\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796ffea",
   "metadata": {},
   "source": [
    "# Get Table object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abdc019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(url, trend):\n",
    "    global table\n",
    "    global soup\n",
    "    data = requests.get(url).text\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'class': 'tbl', 'data-trend-type': trend})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a59e3",
   "metadata": {},
   "source": [
    "# Scrape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ed2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, trend):\n",
    "    global locality_name\n",
    "    global price_range_per_sq_ft\n",
    "    global avg_price_per_sq_ft\n",
    "    global price_rise\n",
    "    global city\n",
    "    global city_url\n",
    "    global page_numbers\n",
    "    global page_links\n",
    "    global total_pages\n",
    "    \n",
    "    locality_name = []\n",
    "    price_range_per_sq_ft = []\n",
    "    avg_price_per_sq_ft = []\n",
    "    price_rise = []\n",
    "    city = []\n",
    "    city_url = []\n",
    "    page_numbers = []\n",
    "\n",
    "\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(url + f\"?page={page}\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "\n",
    "        table = soup.find('table', {'class': 'tbl', 'data-trend-type': trend})\n",
    "        if table:\n",
    "            for row in table.tbody.find_all('tr'):\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) > 4:\n",
    "                    first_column = columns[0].get_text(strip=True)\n",
    "                    locality_name.append(first_column)\n",
    "\n",
    "                    second_column = columns[1].get_text(strip=True)\n",
    "                    price_range_per_sq_ft.append(second_column)\n",
    "\n",
    "                    third_column = columns[2].get_text(strip=True)\n",
    "                    avg_price_per_sq_ft.append(third_column)\n",
    "\n",
    "                    fourth_column = columns[3].get_text(strip=True)\n",
    "                    price_rise.append(fourth_column)\n",
    "\n",
    "                    city_element = soup.find('h1', {'class': 'page-hdng'})\n",
    "                    city_name = city_element.text.strip()\n",
    "                    city_name = city_name.split(' in ')[-1].split(' - ')[0]\n",
    "\n",
    "                    city.append(city_name)\n",
    "                    city_url.append(url)\n",
    "                    page_numbers.append(page)\n",
    "\n",
    "            pagination = soup.find('div', class_='pagination')\n",
    "            \n",
    "            if not pagination:\n",
    "                total_pages = 1\n",
    "                break\n",
    "\n",
    "            page_links = pagination.find_all('a')\n",
    "            if len(page_links) < 2:\n",
    "                break\n",
    "\n",
    "            last_page_link = page_links[-2].get('href')\n",
    "            if last_page_link is None:\n",
    "                total_pages = 1\n",
    "                break\n",
    "                \n",
    "            total_pages = int(last_page_link.split('=')[-1])+1\n",
    "            if page >= total_pages:\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce843677",
   "metadata": {},
   "source": [
    "# Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538b3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data():\n",
    "    global FLOAT_avg_price_per_sq_ft\n",
    "    global INT_price_rise\n",
    "    global converted_data\n",
    "    \n",
    "    FLOAT_avg_price_per_sq_ft = []\n",
    "    INT_price_rise = []\n",
    "    converted_data = []\n",
    "\n",
    "    FLOAT_avg_price_per_sq_ft.clear()\n",
    "    INT_price_rise.clear()\n",
    "    converted_data.clear()\n",
    "\n",
    "\n",
    "    #1\n",
    "    for price in avg_price_per_sq_ft:\n",
    "        if price == '-':\n",
    "            FLOAT_avg_price_per_sq_ft.append(None)\n",
    "        else:\n",
    "            cleaned_price = re.sub(r'[^\\d.]', '', price)\n",
    "            FLOAT_avg_price_per_sq_ft.append(float(cleaned_price) if cleaned_price else None)\n",
    "\n",
    "\n",
    "    #2        \n",
    "    for data in price_rise:\n",
    "        if data == '-':\n",
    "            INT_price_rise.append(None)\n",
    "        else:\n",
    "            cleaned_data = re.sub(r'[^-?\\d.]', '', data)\n",
    "            if cleaned_data:\n",
    "                rounded_value = math.copysign(round(abs(float(cleaned_data))), float(cleaned_data))\n",
    "                INT_price_rise.append(rounded_value)\n",
    "            else:\n",
    "                INT_price_rise.append(None)\n",
    "    \n",
    "\n",
    "    #3\n",
    "    for data in price_range_per_sq_ft:\n",
    "        range_match = re.search(r'(\\d+(?:,\\d+)?)(?:-(\\d+(?:,\\d+)?))?', data)\n",
    "        if range_match:\n",
    "            min_value = int(range_match.group(1).replace(',', ''))\n",
    "            max_value = int(range_match.group(2).replace(',', '')) if range_match.group(2) else min_value\n",
    "            #converted_data.append((min_value, max_value))\n",
    "            converted_data.append(f\"{min_value}-{max_value}\")\n",
    "        else:\n",
    "            converted_data.append(None)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822abef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5890790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(i):\n",
    "    global df\n",
    "    global INT_price_rise2\n",
    "    INT_price_rise2 = [int(x) if x is not None else None for x in INT_price_rise]    \n",
    "    \n",
    "    df_name = f\"df{i}\"  # Generate the variable name\n",
    "    globals()[df_name] = pd.DataFrame({'city_name': city, 'city_url': city_url,'locality_name': locality_name, 'price_range_per_sq_ft': converted_data, 'avg_price_per_sq_ft': FLOAT_avg_price_per_sq_ft, 'price_rise': INT_price_rise2, 'page_number': page_numbers})  \n",
    "    \n",
    "    return globals()[df_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43ebf1",
   "metadata": {},
   "source": [
    "# Calling the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "950159a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apartment\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Chennai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Mumbai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Pune\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Puri\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Bangalore\n",
      "builderfloor\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Chennai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Mumbai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Pune\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Puri\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Bangalore\n",
      "villa\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Chennai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Mumbai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Pune\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Puri\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Bangalore\n",
      "plot\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Chennai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Mumbai\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Pune\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Puri\n",
      "https://www.makaan.com/price-trends/property-rates-for-buy-in-Bangalore\n"
     ]
    }
   ],
   "source": [
    "trend = ['apartment', 'builderfloor', 'villa', 'plot']\n",
    "urls = get_urls()\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "df_list = []\n",
    "total_pages_ls = []\n",
    "locality_name_len = []\n",
    "\n",
    "df_list.clear()\n",
    "total_pages_ls.clear()\n",
    "locality_name_len.clear()\n",
    "\n",
    "# creating connection with SQLLite\n",
    "conn = sqlite3.connect('makaan_locality_prices.db')\n",
    "\n",
    "for tr in trend:\n",
    "    print(tr)\n",
    "    df_list.clear()\n",
    "    for i, url in enumerate(urls):\n",
    "        print(url)\n",
    "        \n",
    "        get_table(url, tr)\n",
    "        get_data(url, tr)\n",
    "        clean_data()\n",
    "        \n",
    "        #df = df.iloc[0:0]\n",
    "        df = extract_data(i+1)\n",
    "        df_list.append(df)\n",
    "        \n",
    "        if tr == 'apartment':\n",
    "            total_pages_ls.append(total_pages)\n",
    "            locality_name_len.append(len(locality_name))   \n",
    "        \n",
    "    new_df = pd.concat(df_list)\n",
    "    new_df['price_rise'] = new_df['price_rise'].astype('Int64')\n",
    "    \n",
    "    # insert column using insert(position,column_name,first_column) function\n",
    "    first_column = new_df.pop('page_number')\n",
    "    new_df.insert(0, 'page_number', first_column)\n",
    "    \n",
    "    new_df.to_sql(tr, conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Generating our main file\n",
    "    if os.path.isfile('makaan_locality_prices.xlsx'):\n",
    "        with pd.ExcelWriter('makaan_locality_prices.xlsx', mode='a', engine='openpyxl', if_sheet_exists=\"replace\") as writer:\n",
    "            new_df.to_excel(writer, index=False, sheet_name=tr)\n",
    "        \n",
    "    else:   \n",
    "        new_df.to_excel('makaan_locality_prices.xlsx', index=False, sheet_name=tr)\n",
    "        \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f9b58",
   "metadata": {},
   "source": [
    "# Generating 'makaan_localities.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b48246",
   "metadata": {},
   "outputs": [],
   "source": [
    "dk = pd.DataFrame({'city_name': city_names, 'city_url': urls,'total_pages': total_pages_ls, 'total_localities': locality_name_len})\n",
    "dk.to_csv('makaan_localities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0161778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
